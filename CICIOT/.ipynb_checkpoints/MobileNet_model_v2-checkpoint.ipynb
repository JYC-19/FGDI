{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "811a93b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "\n",
    "def _make_divisible(ch, divisor=8, min_ch=None):\n",
    "    if min_ch is None:\n",
    "        min_ch = divisor\n",
    "    new_ch = max(min_ch, int(ch + divisor / 2) // divisor * divisor)\n",
    "    if new_ch < 0.9 * ch:\n",
    "        new_ch += divisor\n",
    "    return new_ch\n",
    "\n",
    "\n",
    "class ConvBNReLU(nn.Sequential):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size=3, stride=1, groups=1):\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        super(ConvBNReLU, self).__init__(\n",
    "            nn.Conv1d(in_channel, out_channel, kernel_size, stride, padding, groups=groups, bias=False),\n",
    "            nn.BatchNorm1d(out_channel),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        hidden_channel = in_channel * expand_ratio\n",
    "        self.use_shortcut = stride == 1 and in_channel == out_channel\n",
    "\n",
    "        layers = []\n",
    "        if expand_ratio != 1:\n",
    "            layers.append(ConvBNReLU(in_channel, hidden_channel, kernel_size=1))\n",
    "        layers.extend([\n",
    "            ConvBNReLU(hidden_channel, hidden_channel, stride=stride, groups=hidden_channel),\n",
    "            nn.Conv1d(hidden_channel, out_channel, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm1d(out_channel),\n",
    "        ])\n",
    "\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_shortcut:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes=40, alpha=1.0, round_nearest=8):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = _make_divisible(32 * alpha, round_nearest)\n",
    "        last_channel = _make_divisible(1280 * alpha, round_nearest)\n",
    "\n",
    "        inverted_residual_setting = [\n",
    "            # t, c, n, s\n",
    "            [1, 16, 1, 1],\n",
    "            [6, 24, 2, 2],\n",
    "            [6, 32, 3, 2],\n",
    "            [6, 64, 4, 2],\n",
    "            [6, 96, 3, 1],\n",
    "            [6, 160, 3, 2],\n",
    "            [6, 320, 1, 1],\n",
    "        ]\n",
    "\n",
    "        features = []\n",
    "        features.append(ConvBNReLU(1, input_channel, stride=2))\n",
    "        for t, c, n, s in inverted_residual_setting:\n",
    "            output_channel = _make_divisible(c * alpha, round_nearest)\n",
    "            for i in range(n):\n",
    "                stride = s if i == 0 else 1\n",
    "                features.append(block(input_channel, output_channel, stride, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "        features.append(ConvBNReLU(input_channel, last_channel, 1))\n",
    "        self.features = nn.Sequential(*features)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(last_channel, num_classes)\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "198d86dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_matrix(conf_matrix, dev_list, save_path):\n",
    "    plt.figure(figsize=(20, 16))\n",
    "    plt.imshow(conf_matrix, cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "\n",
    "    thresh = conf_matrix.max() / 2.\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        for j in range(conf_matrix.shape[1]):\n",
    "            plt.text(j, i, conf_matrix[i, j],\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if conf_matrix[i, j] > thresh else \"black\", fontsize=6)\n",
    "\n",
    "    tick_marks = np.arange(len(dev_list))\n",
    "    plt.xticks(tick_marks, dev_list)\n",
    "    plt.yticks(tick_marks, dev_list)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24fa0288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda:0 device.\n",
      "using 312067 images for training, 78016 images for validation.\n",
      "train loss:100%[**************************************************->]3.485\n",
      "[epoch 1] train_loss: 0.723 train_accuracy: 0.793 val_accuracy: 0.124  recall: 0.121  f1: 0.109\n",
      "train loss:100%[**************************************************->]1.950\n",
      "[epoch 2] train_loss: 0.250 train_accuracy: 0.920 val_accuracy: 0.084  recall: 0.082  f1: 0.078\n",
      "train loss:100%[**************************************************->]3.050\n",
      "[epoch 3] train_loss: 0.172 train_accuracy: 0.944 val_accuracy: 0.279  recall: 0.272  f1: 0.247\n",
      "train loss:100%[**************************************************->]6.063\n",
      "[epoch 4] train_loss: 0.133 train_accuracy: 0.955 val_accuracy: 0.368  recall: 0.359  f1: 0.376\n",
      "train loss:100%[**************************************************->]0.711\n",
      "[epoch 5] train_loss: 0.107 train_accuracy: 0.964 val_accuracy: 0.281  recall: 0.274  f1: 0.316\n",
      "train loss:100%[**************************************************->]0.068\n",
      "[epoch 6] train_loss: 0.088 train_accuracy: 0.970 val_accuracy: 0.087  recall: 0.085  f1: 0.083\n",
      "train loss:100%[**************************************************->]0.072\n",
      "[epoch 7] train_loss: 0.074 train_accuracy: 0.974 val_accuracy: 0.178  recall: 0.174  f1: 0.164\n",
      "train loss:100%[**************************************************->]0.462\n",
      "[epoch 8] train_loss: 0.063 train_accuracy: 0.978 val_accuracy: 0.847  recall: 0.826  f1: 0.834\n",
      "train loss:100%[**************************************************->]0.312\n",
      "[epoch 9] train_loss: 0.055 train_accuracy: 0.981 val_accuracy: 0.291  recall: 0.283  f1: 0.283\n",
      "train loss:100%[**************************************************->]1.091\n",
      "[epoch 10] train_loss: 0.050 train_accuracy: 0.982 val_accuracy: 0.973  recall: 0.973  f1: 0.973\n",
      "train loss:100%[**************************************************->]0.040\n",
      "[epoch 11] train_loss: 0.045 train_accuracy: 0.984 val_accuracy: 0.151  recall: 0.147  f1: 0.156\n",
      "train loss:100%[**************************************************->]1.732\n",
      "[epoch 12] train_loss: 0.041 train_accuracy: 0.985 val_accuracy: 0.145  recall: 0.141  f1: 0.137\n",
      "train loss:100%[**************************************************->]0.017\n",
      "[epoch 13] train_loss: 0.037 train_accuracy: 0.986 val_accuracy: 0.115  recall: 0.112  f1: 0.091\n",
      "train loss:100%[**************************************************->]0.011\n",
      "[epoch 14] train_loss: 0.035 train_accuracy: 0.987 val_accuracy: 0.545  recall: 0.532  f1: 0.537\n",
      "train loss:100%[**************************************************->]0.053\n",
      "[epoch 15] train_loss: 0.034 train_accuracy: 0.988 val_accuracy: 0.088  recall: 0.085  f1: 0.084\n",
      "train loss:100%[**************************************************->]0.250\n",
      "[epoch 16] train_loss: 0.032 train_accuracy: 0.988 val_accuracy: 0.913  recall: 0.891  f1: 0.891\n",
      "train loss:100%[**************************************************->]0.592\n",
      "[epoch 17] train_loss: 0.030 train_accuracy: 0.989 val_accuracy: 0.142  recall: 0.138  f1: 0.135\n",
      "train loss:100%[**************************************************->]0.776\n",
      "[epoch 18] train_loss: 0.029 train_accuracy: 0.990 val_accuracy: 0.444  recall: 0.433  f1: 0.417\n",
      "train loss:100%[**************************************************->]5.172\n",
      "[epoch 19] train_loss: 0.027 train_accuracy: 0.990 val_accuracy: 0.095  recall: 0.093  f1: 0.095\n",
      "train loss:100%[**************************************************->]0.144\n",
      "[epoch 20] train_loss: 0.026 train_accuracy: 0.990 val_accuracy: 0.127  recall: 0.124  f1: 0.124\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.categories = sorted(os.listdir(data_dir))\n",
    "        self.data = []\n",
    "        self.transform = transform\n",
    "        for category in self.categories:\n",
    "            category_dir = os.path.join(data_dir, category)\n",
    "            category_data = sorted(os.listdir(category_dir))\n",
    "            self.data.extend([(os.path.join(category_dir, file), self.categories.index(category)) for file in category_data])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_path, label = self.data[index]\n",
    "        data = np.load(file_path)\n",
    "        image = Image.fromarray(data.astype(np.uint8))\n",
    "        image = transform(image)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using {} device.\".format(device))\n",
    "\n",
    "transform = transforms.Compose([ transforms.Grayscale(num_output_channels = 1),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5), (0.5))])\n",
    "\n",
    "train_dataset = CustomDataset(\"features/train_npy\",transform=transform)\n",
    "train_num = len(train_dataset)\n",
    "dev_list = train_dataset.categories\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size, shuffle=True,\n",
    "                                               num_workers=0)\n",
    "\n",
    "validate_dataset = CustomDataset(\"features/val_npy\",transform=transform)\n",
    "val_num = len(validate_dataset)\n",
    "validate_loader = torch.utils.data.DataLoader(validate_dataset,\n",
    "                                                  batch_size=batch_size, shuffle=False,\n",
    "                                                  num_workers=0)\n",
    "\n",
    "print(\"using {} images for training, {} images for validation.\".format(train_num, val_num))\n",
    "\n",
    "net = MobileNetV2(num_classes=40)\n",
    "net.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "epochs = 20\n",
    "save_path = './MobileNet_parameters.pth'\n",
    "best_f1 = 0.0\n",
    "train_accurate_list = []\n",
    "val_accurate_list = []\n",
    "f1_list = []\n",
    "recall_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    for step, data in enumerate(train_loader, start=0):\n",
    "        images, labels = data\n",
    "        images = images.reshape(images.shape[0], 1, 1500)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images.to(device))\n",
    "        predict_y = torch.max(outputs, dim=1)[1]\n",
    "        train_acc += torch.eq(predict_y, labels.to(device)).sum().item()\n",
    "        loss = loss_function(outputs,labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        rate = (step + 1) / len(train_loader)\n",
    "        a = \"*\" * int(rate * 50)\n",
    "        b = \".\" * int((1 - rate) * 50)\n",
    "        print(\"\\rtrain loss:{:^3.0f}%[{}->{}]{:.3f}\".format(int(rate * 100), a, b, loss), end=\"\")\n",
    "    print()\n",
    "    train_accurate = train_acc / train_num\n",
    "    train_accurate_list.append(train_accurate)\n",
    "    net.eval()\n",
    "    acc = 0.0  \n",
    "    val = torch.tensor([])\n",
    "    pre = torch.tensor([])\n",
    "    with torch.no_grad():\n",
    "        for val_data in validate_loader:\n",
    "            val_images, val_labels = val_data\n",
    "            val_images = val_images.reshape(val_images.shape[0], 1, 1500)\n",
    "            outputs = net(val_images.to(device))\n",
    "            predict_y = torch.max(outputs, dim=1)[1]\n",
    "            pre = torch.cat([pre.to(device), predict_y.to(device)])\n",
    "            val = torch.cat([val.to(device), val_labels.to(device)])\n",
    "            acc += torch.eq(predict_y, val_labels.to(device)).sum().item()\n",
    "    val_accurate = acc / val_num\n",
    "    val_accurate_list.append(val_accurate)\n",
    "    f1 = f1_score(val.cpu(), pre.cpu(), average='macro')\n",
    "    recall = recall_score(val.cpu(), pre.cpu(), average='macro')\n",
    "\n",
    "    f1_list.append(f1)\n",
    "    recall_list.append(recall)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_pre = pre\n",
    "        best_val = val\n",
    "        torch.save(net.state_dict(), save_path)\n",
    "        torch.save(best_pre, 'pre_val_label/best_pre_MobileNet.pt')\n",
    "        torch.save(best_val, 'pre_val_label/best_val_MobileNet.pt')\n",
    "    print('[epoch %d] train_loss: %.3f train_accuracy: %.3f val_accuracy: %.3f  recall: %.3f  f1: %.3f' %\n",
    "          (epoch + 1, running_loss / step, train_accurate, val_accurate, recall, f1))\n",
    "    with open(\"MobileNet_result_npy.txt\", 'a') as file:\n",
    "        file.write(\"[epoch \" + str(epoch + 1) + \"]\" + \"  \" + \"train_accuracy:\" + str(train_accurate) + \"  \" + \"val_accuracy:\" + str(val_accurate) + \"  \" + \"recall:\" + str(recall) + \"  \" + \"f1:\" + str(f1) + '\\n')\n",
    "print('Finished Training')\n",
    "iterations = range(1, len(train_accurate_list) + 1)\n",
    "with open(\"MobileNet_npy_plt_data.txt\", 'a') as file:\n",
    "    file.write(\"iterations:\" + str(iterations) +\n",
    "               \"train_accurate_list:\" + str(train_accurate_list) +\n",
    "               \"val_accurate_list:\" + str(val_accurate_list) +\n",
    "               \"f1_list:\" + str(f1_list) +\n",
    "               \"recall_list:\" + str(recall_list) +\n",
    "               \"dev_list:\" + str(dev_list) + '\\n')\n",
    "conf_matrix = confusion_matrix(best_val.cpu(),best_pre.cpu())\n",
    "plot_matrix(conf_matrix,dev_list,\"MobileNet_confusion_matrix_npy.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99e8f58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
