{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e3df19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GoogLeNet(nn.Module):\n",
    "    def __init__(self, num_classes=40, aux_logits=True, init_weights=True):\n",
    "        super(GoogLeNet,self).__init__()\n",
    "        self.aux_logits = aux_logits\n",
    "\n",
    "        self.conv1 = BasicConv1d(1, 64, kernel_size=7, stride=2, padding=3)    \n",
    "        self.maxpool1 = nn.MaxPool1d(3, stride=2, ceil_mode=True)              \n",
    "\n",
    "        self.conv2 = BasicConv1d(64, 64, kernel_size=1)\n",
    "        self.conv3 = BasicConv1d(64, 192, kernel_size=3, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool1d(3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception3a = Inception(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.inception3b = Inception(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.maxpool3 = nn.MaxPool1d(3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception4a = Inception(480, 192, 96, 208, 16, 48, 64)\n",
    "        self.inception4b = Inception(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.inception4c = Inception(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.inception4d = Inception(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.inception4e = Inception(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.maxpool4 = nn.MaxPool1d(3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception5a = Inception(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.inception5b = Inception(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "        if self.aux_logits:\n",
    "            self.aux1 = InceptionAux(512, num_classes)\n",
    "            self.aux2 = InceptionAux(528, num_classes)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = self.inception3a(x)\n",
    "        x = self.inception3b(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.inception4a(x)\n",
    "        if self.training and self.aux_logits: \n",
    "            aux1 = self.aux1(x)\n",
    "\n",
    "        x = self.inception4b(x)\n",
    "        x = self.inception4c(x)\n",
    "        x = self.inception4d(x)\n",
    "        if self.training and self.aux_logits:  \n",
    "            aux2 = self.aux2(x)\n",
    "\n",
    "        x = self.inception4e(x)\n",
    "        x = self.maxpool4(x)\n",
    "        x = self.inception5a(x)\n",
    "        x = self.inception5b(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        if self.training and self.aux_logits: \n",
    "            return x, aux2, aux1\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "class Inception(nn.Module):\n",
    "    def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj):\n",
    "        super(Inception,self).__init__()\n",
    "\n",
    "        self.branch1 = BasicConv1d(in_channels, ch1x1, kernel_size=1)\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv1d(in_channels, ch3x3red, kernel_size=1),\n",
    "            BasicConv1d(ch3x3red, ch3x3, kernel_size=3, padding=1)         \n",
    "        )\n",
    "\n",
    "        self.branch3 = nn.Sequential(\n",
    "            BasicConv1d(in_channels, ch5x5red, kernel_size=1),\n",
    "            BasicConv1d(ch5x5red, ch5x5, kernel_size=5, padding=2)        \n",
    "        )\n",
    "\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1, padding=1),              \n",
    "            BasicConv1d(in_channels, pool_proj, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "        branch4 = self.branch4(x)\n",
    "\n",
    "        outputs = [branch1, branch2, branch3, branch4]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "class InceptionAux(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(InceptionAux,self).__init__()\n",
    "        self.averagePool = nn.AvgPool1d(kernel_size=5, stride=3)\n",
    "        self.conv = BasicConv1d(in_channels, 128, kernel_size=1)        \n",
    "\n",
    "        self.fc1 = nn.Linear(3840, 1024)\n",
    "        self.fc2 = nn.Linear(1024,num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.averagePool(x)\n",
    "        x = self.conv(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = F.dropout(x, 0.5, training=self.training)\n",
    "        x = F.relu(self.fc1(x), inplace=True)\n",
    "        x = F.dropout(x, 0.5, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class BasicConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(BasicConv1d, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, **kwargs)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c013861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_matrix(conf_matrix, dev_list, save_path):\n",
    "    plt.figure(figsize=(20, 16), dpi=300)\n",
    "    plt.imshow(conf_matrix, cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "\n",
    "    thresh = conf_matrix.max() / 2.\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        for j in range(conf_matrix.shape[1]):\n",
    "            plt.text(j, i, conf_matrix[i, j],\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if conf_matrix[i, j] > thresh else \"black\", fontsize=6)\n",
    "\n",
    "    tick_marks = np.arange(len(dev_list))\n",
    "    plt.xticks(tick_marks, dev_list)\n",
    "    plt.yticks(tick_marks, dev_list)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39421773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda:0 device.\n",
      "using 86137 images for training, 21515 images for validation.\n",
      "train loss:100%[**************************************************->]2.482\n",
      "[epoch 1] train_loss: 3.608 train_accuracy: 0.307 val_accuracy: 0.431  recall: 0.144  f1: 0.128\n",
      "train loss:100%[**************************************************->]1.663\n",
      "[epoch 2] train_loss: 2.459 train_accuracy: 0.490 val_accuracy: 0.525  recall: 0.246  f1: 0.231\n",
      "train loss:100%[**************************************************->]1.647\n",
      "[epoch 3] train_loss: 1.992 train_accuracy: 0.562 val_accuracy: 0.597  recall: 0.368  f1: 0.367\n",
      "train loss:100%[**************************************************->]1.354\n",
      "[epoch 4] train_loss: 1.726 train_accuracy: 0.612 val_accuracy: 0.643  recall: 0.479  f1: 0.446\n",
      "train loss:100%[**************************************************->]1.527\n",
      "[epoch 5] train_loss: 1.512 train_accuracy: 0.658 val_accuracy: 0.687  recall: 0.542  f1: 0.533\n",
      "train loss:100%[**************************************************->]1.139\n",
      "[epoch 6] train_loss: 1.361 train_accuracy: 0.693 val_accuracy: 0.701  recall: 0.564  f1: 0.550\n",
      "train loss:100%[**************************************************->]1.433\n",
      "[epoch 7] train_loss: 1.256 train_accuracy: 0.714 val_accuracy: 0.734  recall: 0.599  f1: 0.611\n",
      "train loss:100%[**************************************************->]0.940\n",
      "[epoch 8] train_loss: 1.174 train_accuracy: 0.732 val_accuracy: 0.742  recall: 0.610  f1: 0.606\n",
      "train loss:100%[**************************************************->]1.017\n",
      "[epoch 9] train_loss: 1.104 train_accuracy: 0.748 val_accuracy: 0.751  recall: 0.655  f1: 0.652\n",
      "train loss:100%[**************************************************->]1.095\n",
      "[epoch 10] train_loss: 1.046 train_accuracy: 0.760 val_accuracy: 0.759  recall: 0.675  f1: 0.671\n",
      "train loss:100%[**************************************************->]1.043\n",
      "[epoch 11] train_loss: 0.997 train_accuracy: 0.773 val_accuracy: 0.787  recall: 0.689  f1: 0.710\n",
      "train loss:100%[**************************************************->]0.947\n",
      "[epoch 12] train_loss: 0.954 train_accuracy: 0.783 val_accuracy: 0.784  recall: 0.689  f1: 0.704\n",
      "train loss:100%[**************************************************->]0.510\n",
      "[epoch 13] train_loss: 0.909 train_accuracy: 0.793 val_accuracy: 0.807  recall: 0.723  f1: 0.740\n",
      "train loss:100%[**************************************************->]1.365\n",
      "[epoch 14] train_loss: 0.873 train_accuracy: 0.804 val_accuracy: 0.792  recall: 0.688  f1: 0.704\n",
      "train loss:100%[**************************************************->]0.833\n",
      "[epoch 15] train_loss: 0.841 train_accuracy: 0.811 val_accuracy: 0.820  recall: 0.739  f1: 0.750\n",
      "train loss:100%[**************************************************->]0.775\n",
      "[epoch 16] train_loss: 0.816 train_accuracy: 0.816 val_accuracy: 0.818  recall: 0.727  f1: 0.733\n",
      "train loss:100%[**************************************************->]0.503\n",
      "[epoch 17] train_loss: 0.788 train_accuracy: 0.825 val_accuracy: 0.822  recall: 0.730  f1: 0.740\n",
      "train loss:100%[**************************************************->]0.469\n",
      "[epoch 18] train_loss: 0.765 train_accuracy: 0.831 val_accuracy: 0.827  recall: 0.753  f1: 0.767\n",
      "train loss:100%[**************************************************->]0.533\n",
      "[epoch 19] train_loss: 0.736 train_accuracy: 0.836 val_accuracy: 0.834  recall: 0.774  f1: 0.789\n",
      "train loss:100%[**************************************************->]0.613\n",
      "[epoch 20] train_loss: 0.713 train_accuracy: 0.843 val_accuracy: 0.842  recall: 0.771  f1: 0.788\n",
      "train loss:100%[**************************************************->]0.534\n",
      "[epoch 21] train_loss: 0.689 train_accuracy: 0.848 val_accuracy: 0.842  recall: 0.764  f1: 0.769\n",
      "train loss:100%[**************************************************->]0.670\n",
      "[epoch 22] train_loss: 0.665 train_accuracy: 0.855 val_accuracy: 0.851  recall: 0.771  f1: 0.770\n",
      "train loss:100%[**************************************************->]0.562\n",
      "[epoch 23] train_loss: 0.639 train_accuracy: 0.862 val_accuracy: 0.859  recall: 0.777  f1: 0.793\n",
      "train loss:100%[**************************************************->]0.538\n",
      "[epoch 24] train_loss: 0.610 train_accuracy: 0.870 val_accuracy: 0.858  recall: 0.797  f1: 0.806\n",
      "train loss:100%[**************************************************->]0.433\n",
      "[epoch 25] train_loss: 0.584 train_accuracy: 0.876 val_accuracy: 0.871  recall: 0.804  f1: 0.820\n",
      "train loss:100%[**************************************************->]0.340\n",
      "[epoch 26] train_loss: 0.562 train_accuracy: 0.882 val_accuracy: 0.878  recall: 0.812  f1: 0.828\n",
      "train loss:100%[**************************************************->]1.215\n",
      "[epoch 27] train_loss: 0.543 train_accuracy: 0.887 val_accuracy: 0.873  recall: 0.811  f1: 0.821\n",
      "train loss:100%[**************************************************->]0.692\n",
      "[epoch 28] train_loss: 0.516 train_accuracy: 0.893 val_accuracy: 0.887  recall: 0.823  f1: 0.841\n",
      "train loss:100%[**************************************************->]0.622\n",
      "[epoch 29] train_loss: 0.499 train_accuracy: 0.898 val_accuracy: 0.894  recall: 0.832  f1: 0.853\n",
      "train loss:100%[**************************************************->]0.259\n",
      "[epoch 30] train_loss: 0.483 train_accuracy: 0.902 val_accuracy: 0.901  recall: 0.833  f1: 0.856\n",
      "train loss:100%[**************************************************->]0.247\n",
      "[epoch 31] train_loss: 0.467 train_accuracy: 0.907 val_accuracy: 0.901  recall: 0.836  f1: 0.851\n",
      "train loss:100%[**************************************************->]0.328\n",
      "[epoch 32] train_loss: 0.446 train_accuracy: 0.912 val_accuracy: 0.904  recall: 0.836  f1: 0.844\n",
      "train loss:100%[**************************************************->]0.499\n",
      "[epoch 33] train_loss: 0.432 train_accuracy: 0.914 val_accuracy: 0.900  recall: 0.830  f1: 0.847\n",
      "train loss:100%[**************************************************->]0.883\n",
      "[epoch 34] train_loss: 0.419 train_accuracy: 0.918 val_accuracy: 0.897  recall: 0.817  f1: 0.830\n",
      "train loss:100%[**************************************************->]0.152\n",
      "[epoch 35] train_loss: 0.404 train_accuracy: 0.921 val_accuracy: 0.908  recall: 0.840  f1: 0.860\n",
      "train loss:100%[**************************************************->]0.557\n",
      "[epoch 36] train_loss: 0.388 train_accuracy: 0.925 val_accuracy: 0.914  recall: 0.831  f1: 0.855\n",
      "train loss:100%[**************************************************->]0.221\n",
      "[epoch 37] train_loss: 0.381 train_accuracy: 0.927 val_accuracy: 0.918  recall: 0.859  f1: 0.869\n",
      "train loss:100%[**************************************************->]0.373\n",
      "[epoch 38] train_loss: 0.359 train_accuracy: 0.932 val_accuracy: 0.915  recall: 0.848  f1: 0.861\n",
      "train loss:100%[**************************************************->]1.175\n",
      "[epoch 39] train_loss: 0.359 train_accuracy: 0.932 val_accuracy: 0.919  recall: 0.844  f1: 0.862\n",
      "train loss:100%[**************************************************->]0.234\n",
      "[epoch 40] train_loss: 0.346 train_accuracy: 0.935 val_accuracy: 0.920  recall: 0.846  f1: 0.864\n",
      "train loss:100%[**************************************************->]0.139\n",
      "[epoch 41] train_loss: 0.335 train_accuracy: 0.937 val_accuracy: 0.919  recall: 0.857  f1: 0.870\n",
      "train loss:100%[**************************************************->]0.090\n",
      "[epoch 42] train_loss: 0.329 train_accuracy: 0.939 val_accuracy: 0.911  recall: 0.847  f1: 0.860\n",
      "train loss:100%[**************************************************->]0.793\n",
      "[epoch 43] train_loss: 0.315 train_accuracy: 0.941 val_accuracy: 0.918  recall: 0.835  f1: 0.844\n",
      "train loss:100%[**************************************************->]0.094\n",
      "[epoch 44] train_loss: 0.307 train_accuracy: 0.943 val_accuracy: 0.931  recall: 0.866  f1: 0.885\n",
      "train loss:100%[**************************************************->]0.266\n",
      "[epoch 45] train_loss: 0.297 train_accuracy: 0.945 val_accuracy: 0.929  recall: 0.858  f1: 0.867\n",
      "train loss:100%[**************************************************->]0.410\n",
      "[epoch 46] train_loss: 0.293 train_accuracy: 0.946 val_accuracy: 0.923  recall: 0.839  f1: 0.851\n",
      "train loss:100%[**************************************************->]0.185\n",
      "[epoch 47] train_loss: 0.286 train_accuracy: 0.948 val_accuracy: 0.928  recall: 0.865  f1: 0.872\n",
      "train loss:100%[**************************************************->]0.167\n",
      "[epoch 48] train_loss: 0.275 train_accuracy: 0.949 val_accuracy: 0.936  recall: 0.867  f1: 0.874\n",
      "train loss:100%[**************************************************->]0.573\n",
      "[epoch 49] train_loss: 0.271 train_accuracy: 0.951 val_accuracy: 0.933  recall: 0.877  f1: 0.885\n",
      "train loss:100%[**************************************************->]0.396\n",
      "[epoch 50] train_loss: 0.262 train_accuracy: 0.952 val_accuracy: 0.931  recall: 0.871  f1: 0.888\n",
      "train loss:100%[**************************************************->]0.275\n",
      "[epoch 51] train_loss: 0.256 train_accuracy: 0.954 val_accuracy: 0.936  recall: 0.875  f1: 0.886\n",
      "train loss:100%[**************************************************->]0.132\n",
      "[epoch 52] train_loss: 0.253 train_accuracy: 0.955 val_accuracy: 0.932  recall: 0.866  f1: 0.873\n",
      "train loss:100%[**************************************************->]0.214\n",
      "[epoch 53] train_loss: 0.247 train_accuracy: 0.955 val_accuracy: 0.934  recall: 0.869  f1: 0.880\n",
      "train loss:100%[**************************************************->]0.229\n",
      "[epoch 54] train_loss: 0.239 train_accuracy: 0.957 val_accuracy: 0.929  recall: 0.846  f1: 0.852\n",
      "train loss:100%[**************************************************->]0.045\n",
      "[epoch 55] train_loss: 0.234 train_accuracy: 0.958 val_accuracy: 0.940  recall: 0.880  f1: 0.889\n",
      "train loss:100%[**************************************************->]0.066\n",
      "[epoch 56] train_loss: 0.228 train_accuracy: 0.959 val_accuracy: 0.930  recall: 0.876  f1: 0.881\n",
      "train loss:100%[**************************************************->]0.343\n",
      "[epoch 57] train_loss: 0.227 train_accuracy: 0.959 val_accuracy: 0.944  recall: 0.884  f1: 0.896\n",
      "train loss:100%[**************************************************->]0.459\n",
      "[epoch 58] train_loss: 0.221 train_accuracy: 0.960 val_accuracy: 0.944  recall: 0.878  f1: 0.887\n",
      "train loss:100%[**************************************************->]0.044\n",
      "[epoch 59] train_loss: 0.219 train_accuracy: 0.961 val_accuracy: 0.930  recall: 0.856  f1: 0.867\n",
      "train loss:100%[**************************************************->]0.502\n",
      "[epoch 60] train_loss: 0.216 train_accuracy: 0.961 val_accuracy: 0.944  recall: 0.889  f1: 0.899\n",
      "train loss:100%[**************************************************->]0.232\n",
      "[epoch 61] train_loss: 0.208 train_accuracy: 0.963 val_accuracy: 0.939  recall: 0.873  f1: 0.882\n",
      "train loss:100%[**************************************************->]0.237\n",
      "[epoch 62] train_loss: 0.206 train_accuracy: 0.964 val_accuracy: 0.943  recall: 0.881  f1: 0.887\n",
      "train loss:100%[**************************************************->]0.384\n",
      "[epoch 63] train_loss: 0.202 train_accuracy: 0.963 val_accuracy: 0.941  recall: 0.879  f1: 0.885\n",
      "train loss:100%[**************************************************->]0.092\n",
      "[epoch 64] train_loss: 0.200 train_accuracy: 0.965 val_accuracy: 0.937  recall: 0.862  f1: 0.867\n",
      "train loss:100%[**************************************************->]0.043\n",
      "[epoch 65] train_loss: 0.192 train_accuracy: 0.966 val_accuracy: 0.940  recall: 0.884  f1: 0.891\n",
      "train loss:100%[**************************************************->]0.376\n",
      "[epoch 66] train_loss: 0.195 train_accuracy: 0.966 val_accuracy: 0.943  recall: 0.886  f1: 0.888\n",
      "train loss:100%[**************************************************->]0.406\n",
      "[epoch 67] train_loss: 0.188 train_accuracy: 0.967 val_accuracy: 0.939  recall: 0.886  f1: 0.880\n",
      "train loss:100%[**************************************************->]0.140\n",
      "[epoch 68] train_loss: 0.185 train_accuracy: 0.967 val_accuracy: 0.945  recall: 0.893  f1: 0.905\n",
      "train loss:100%[**************************************************->]0.076\n",
      "[epoch 69] train_loss: 0.183 train_accuracy: 0.968 val_accuracy: 0.936  recall: 0.878  f1: 0.880\n",
      "train loss:100%[**************************************************->]0.192\n",
      "[epoch 70] train_loss: 0.181 train_accuracy: 0.968 val_accuracy: 0.941  recall: 0.874  f1: 0.882\n",
      "train loss:100%[**************************************************->]0.050\n",
      "[epoch 71] train_loss: 0.175 train_accuracy: 0.969 val_accuracy: 0.947  recall: 0.886  f1: 0.889\n",
      "train loss:100%[**************************************************->]0.239\n",
      "[epoch 72] train_loss: 0.176 train_accuracy: 0.969 val_accuracy: 0.940  recall: 0.880  f1: 0.880\n",
      "train loss:100%[**************************************************->]0.136\n",
      "[epoch 73] train_loss: 0.170 train_accuracy: 0.970 val_accuracy: 0.948  recall: 0.892  f1: 0.893\n",
      "train loss:100%[**************************************************->]0.064\n",
      "[epoch 74] train_loss: 0.170 train_accuracy: 0.970 val_accuracy: 0.935  recall: 0.868  f1: 0.873\n",
      "train loss:100%[**************************************************->]0.179\n",
      "[epoch 75] train_loss: 0.170 train_accuracy: 0.971 val_accuracy: 0.948  recall: 0.879  f1: 0.881\n",
      "train loss:100%[**************************************************->]0.191\n",
      "[epoch 76] train_loss: 0.163 train_accuracy: 0.972 val_accuracy: 0.948  recall: 0.894  f1: 0.897\n",
      "train loss:100%[**************************************************->]0.059\n",
      "[epoch 77] train_loss: 0.166 train_accuracy: 0.971 val_accuracy: 0.945  recall: 0.889  f1: 0.891\n",
      "train loss:100%[**************************************************->]0.358\n",
      "[epoch 78] train_loss: 0.159 train_accuracy: 0.972 val_accuracy: 0.943  recall: 0.890  f1: 0.886\n",
      "train loss:100%[**************************************************->]0.223\n",
      "[epoch 79] train_loss: 0.162 train_accuracy: 0.971 val_accuracy: 0.942  recall: 0.883  f1: 0.885\n",
      "train loss:100%[**************************************************->]0.147\n",
      "[epoch 80] train_loss: 0.151 train_accuracy: 0.974 val_accuracy: 0.947  recall: 0.893  f1: 0.897\n",
      "train loss:100%[**************************************************->]0.036\n",
      "[epoch 81] train_loss: 0.152 train_accuracy: 0.973 val_accuracy: 0.939  recall: 0.876  f1: 0.879\n",
      "train loss:100%[**************************************************->]0.036\n",
      "[epoch 82] train_loss: 0.150 train_accuracy: 0.973 val_accuracy: 0.941  recall: 0.889  f1: 0.896\n",
      "train loss:100%[**************************************************->]0.047\n",
      "[epoch 83] train_loss: 0.150 train_accuracy: 0.974 val_accuracy: 0.954  recall: 0.896  f1: 0.904\n",
      "train loss:100%[**************************************************->]0.142\n",
      "[epoch 84] train_loss: 0.148 train_accuracy: 0.974 val_accuracy: 0.945  recall: 0.894  f1: 0.892\n",
      "train loss:100%[**************************************************->]0.334\n",
      "[epoch 85] train_loss: 0.143 train_accuracy: 0.975 val_accuracy: 0.945  recall: 0.898  f1: 0.900\n",
      "train loss:100%[**************************************************->]0.047\n",
      "[epoch 86] train_loss: 0.143 train_accuracy: 0.975 val_accuracy: 0.946  recall: 0.890  f1: 0.901\n",
      "train loss:100%[**************************************************->]0.450\n",
      "[epoch 87] train_loss: 0.141 train_accuracy: 0.975 val_accuracy: 0.948  recall: 0.887  f1: 0.892\n",
      "train loss:100%[**************************************************->]0.068\n",
      "[epoch 88] train_loss: 0.141 train_accuracy: 0.975 val_accuracy: 0.943  recall: 0.881  f1: 0.885\n",
      "train loss:100%[**************************************************->]0.015\n",
      "[epoch 89] train_loss: 0.139 train_accuracy: 0.976 val_accuracy: 0.950  recall: 0.899  f1: 0.901\n",
      "train loss:100%[**************************************************->]0.241\n",
      "[epoch 90] train_loss: 0.136 train_accuracy: 0.977 val_accuracy: 0.936  recall: 0.883  f1: 0.881\n",
      "train loss:100%[**************************************************->]0.242\n",
      "[epoch 91] train_loss: 0.134 train_accuracy: 0.977 val_accuracy: 0.951  recall: 0.902  f1: 0.904\n",
      "train loss:100%[**************************************************->]0.579\n",
      "[epoch 92] train_loss: 0.135 train_accuracy: 0.976 val_accuracy: 0.952  recall: 0.897  f1: 0.902\n",
      "train loss:100%[**************************************************->]0.113\n",
      "[epoch 93] train_loss: 0.127 train_accuracy: 0.978 val_accuracy: 0.950  recall: 0.900  f1: 0.897\n",
      "train loss:100%[**************************************************->]0.378\n",
      "[epoch 94] train_loss: 0.128 train_accuracy: 0.978 val_accuracy: 0.954  recall: 0.894  f1: 0.896\n",
      "train loss:100%[**************************************************->]0.208\n",
      "[epoch 95] train_loss: 0.130 train_accuracy: 0.978 val_accuracy: 0.955  recall: 0.906  f1: 0.910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:100%[**************************************************->]0.117\n",
      "[epoch 96] train_loss: 0.127 train_accuracy: 0.978 val_accuracy: 0.949  recall: 0.901  f1: 0.898\n",
      "train loss:100%[**************************************************->]0.049\n",
      "[epoch 97] train_loss: 0.125 train_accuracy: 0.979 val_accuracy: 0.955  recall: 0.891  f1: 0.887\n",
      "train loss:100%[**************************************************->]0.096\n",
      "[epoch 98] train_loss: 0.124 train_accuracy: 0.979 val_accuracy: 0.950  recall: 0.887  f1: 0.895\n",
      "train loss:100%[**************************************************->]0.235\n",
      "[epoch 99] train_loss: 0.130 train_accuracy: 0.977 val_accuracy: 0.955  recall: 0.904  f1: 0.901\n",
      "train loss:100%[**************************************************->]0.205\n",
      "[epoch 100] train_loss: 0.119 train_accuracy: 0.979 val_accuracy: 0.951  recall: 0.894  f1: 0.897\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.categories = sorted(os.listdir(data_dir))\n",
    "        self.data = []\n",
    "        self.transform = transform\n",
    "        for category in self.categories:\n",
    "            category_dir = os.path.join(data_dir, category)\n",
    "            category_data = sorted(os.listdir(category_dir))\n",
    "            self.data.extend([(os.path.join(category_dir, file), self.categories.index(category)) for file in category_data])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_path, label = self.data[index]\n",
    "        data = np.load(file_path)\n",
    "        image = Image.fromarray(data.astype(np.uint8))\n",
    "        image = transform(image)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using {} device.\".format(device))\n",
    "\n",
    "transform = transforms.Compose([ transforms.Grayscale(num_output_channels = 1),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5), (0.5))])\n",
    "\n",
    "train_dataset = CustomDataset(\"features/train_npy\",transform=transform)\n",
    "train_num = len(train_dataset)\n",
    "dev_list = train_dataset.categories\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size, shuffle=True,\n",
    "                                               num_workers=0)\n",
    "\n",
    "validate_dataset = CustomDataset(\"features/val_npy\",transform=transform)\n",
    "val_num = len(validate_dataset)\n",
    "validate_loader = torch.utils.data.DataLoader(validate_dataset,\n",
    "                                                  batch_size=batch_size, shuffle=False,\n",
    "                                                  num_workers=0)\n",
    "\n",
    "print(\"using {} images for training, {} images for validation.\".format(train_num, val_num))\n",
    "\n",
    "net = GoogLeNet(num_classes=31, aux_logits=True, init_weights=True)\n",
    "net.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "epochs = 100\n",
    "save_path = './GoogLeNet_parameters.pth'\n",
    "best_f1 = 0.0\n",
    "train_accurate_list = []\n",
    "val_accurate_list = []\n",
    "f1_list = []\n",
    "recall_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    for step, data in enumerate(train_loader, start=0):\n",
    "        images, labels = data\n",
    "        images = images.reshape(images.shape[0], 1, 1500)\n",
    "        optimizer.zero_grad()\n",
    "        logits, aux_logits2, aux_logits1 = net(images.to(device))\n",
    "        predict_y = torch.max(logits, dim=1)[1]\n",
    "        train_acc += torch.eq(predict_y, labels.to(device)).sum().item()\n",
    "        loss0 = loss_function(logits, labels.to(device))\n",
    "        loss1 = loss_function(aux_logits1, labels.to(device))\n",
    "        loss2 = loss_function(aux_logits2, labels.to(device))\n",
    "        loss = loss0 + loss1 * 0.3 + loss2 * 0.3\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        rate = (step + 1) / len(train_loader)\n",
    "        a = \"*\" * int(rate * 50)\n",
    "        b = \".\" * int((1 - rate) * 50)\n",
    "        print(\"\\rtrain loss:{:^3.0f}%[{}->{}]{:.3f}\".format(int(rate * 100), a, b, loss), end=\"\")\n",
    "    print()\n",
    "    train_accurate = train_acc / train_num\n",
    "    train_accurate_list.append(train_accurate)\n",
    "    net.eval()\n",
    "    acc = 0.0  \n",
    "    val = torch.tensor([])\n",
    "    pre = torch.tensor([])\n",
    "    with torch.no_grad():\n",
    "        for val_data in validate_loader:\n",
    "            val_images, val_labels = val_data\n",
    "            val_images = val_images.reshape(val_images.shape[0], 1, 1500)\n",
    "            outputs = net(val_images.to(device))\n",
    "            predict_y = torch.max(outputs, dim=1)[1]\n",
    "            pre = torch.cat([pre.to(device), predict_y.to(device)])\n",
    "            val = torch.cat([val.to(device), val_labels.to(device)])\n",
    "            acc += torch.eq(predict_y, val_labels.to(device)).sum().item()\n",
    "    val_accurate = acc / val_num\n",
    "    val_accurate_list.append(val_accurate)\n",
    "    f1 = f1_score(val.cpu(), pre.cpu(), average='macro')\n",
    "    recall = recall_score(val.cpu(), pre.cpu(), average='macro')\n",
    "\n",
    "    f1_list.append(f1)\n",
    "    recall_list.append(recall)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_pre = pre\n",
    "        best_val = val\n",
    "        torch.save(net.state_dict(), save_path)\n",
    "        torch.save(best_pre, 'pre_val_label/best_pre_GoogLeNet.pt')\n",
    "        torch.save(best_val, 'pre_val_label/best_val_GoogLeNet.pt')\n",
    "    print('[epoch %d] train_loss: %.3f train_accuracy: %.3f val_accuracy: %.3f  recall: %.3f  f1: %.3f' %\n",
    "              (epoch + 1, running_loss / step, train_accurate, val_accurate, recall, f1))\n",
    "    with open(\"GoogLeNet_result_npy.txt\", 'a') as file:\n",
    "        file.write(\"[epoch \" + str(epoch + 1) + \"]\" + \"  \" + \"train_accuracy:\" + str(train_accurate) + \"  \" + \"val_accuracy:\" + str(val_accurate) + \"  \" + \"recall:\" + str(recall) + \"  \" + \"f1:\" + str(f1) + '\\n')\n",
    "print('Finished Training')\n",
    "iterations = range(1, len(train_accurate_list) + 1)\n",
    "with open(\"GoogLeNet_npy_plt_data.txt\", 'a') as file:\n",
    "    file.write(\"iterations:\" + str(iterations) +\n",
    "               \"train_accurate_list:\" + str(train_accurate_list) +\n",
    "               \"val_accurate_list:\" + str(val_accurate_list) +\n",
    "               \"f1_list:\" + str(f1_list) +\n",
    "               \"recall_list:\" + str(recall_list) +\n",
    "               \"dev_list:\" + str(dev_list) + '\\n')\n",
    "conf_matrix = confusion_matrix(best_val.cpu(),best_pre.cpu())\n",
    "plot_matrix(conf_matrix,dev_list,\"GoogLeNet_confusion_matrix_npy.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cd3173",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
