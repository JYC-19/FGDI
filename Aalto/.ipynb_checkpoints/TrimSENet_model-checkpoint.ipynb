{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2267cb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class OneDimensional_SE_ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, blocks_num, num_classes=40):\n",
    "        super(OneDimensional_SE_ResNet, self).__init__()\n",
    "        self.in_channel = 64\n",
    "        self.conv1 = nn.Conv1d(1, self.in_channel, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(self.in_channel)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(self.in_channel, self.in_channel, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(self.in_channel)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, blocks_num[0])\n",
    "        self.layer2 = self._make_layer(block, 128, blocks_num[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, blocks_num[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, blocks_num[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)  # output size = (1, 1)\n",
    "        self.fc1 = nn.Linear(512 * block.expansion, num_classes)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def _make_layer(self, block, channel, block_num, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channel != channel * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.in_channel, channel * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(channel * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channel,\n",
    "                            channel,\n",
    "                            downsample=downsample,\n",
    "                            stride=stride))\n",
    "        self.in_channel = channel * block.expansion\n",
    "\n",
    "        for _ in range(1, block_num):\n",
    "            layers.append(block(self.in_channel,\n",
    "                                channel))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, stride=1, downsample=None):\n",
    "\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=in_channel, out_channels=out_channel,\n",
    "                               kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(in_channels=out_channel, out_channels=out_channel,\n",
    "                               kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channel)\n",
    "\n",
    "        self.downsample = downsample\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(out_channel, out_channel // 16, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(out_channel // 16, out_channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        original_out = out\n",
    "        b, c, _ = out.size()\n",
    "        out = self.avg_pool(out).view(b, c)\n",
    "        out = self.fc(out).view(b, c, 1)\n",
    "        out = out * original_out\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "def se_resnet34(num_classes=40):\n",
    "    return OneDimensional_SE_ResNet(ResidualBlock, [3, 3, 3, 3], num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fab8ed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_matrix(conf_matrix, dev_list, save_path):\n",
    "    plt.figure(figsize=(20, 16), dpi=300)\n",
    "    plt.imshow(conf_matrix, cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "\n",
    "    thresh = conf_matrix.max() / 2.\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        for j in range(conf_matrix.shape[1]):\n",
    "            plt.text(j, i, conf_matrix[i, j],\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if conf_matrix[i, j] > thresh else \"black\", fontsize=6)\n",
    "\n",
    "    tick_marks = np.arange(len(dev_list))\n",
    "    plt.xticks(tick_marks, dev_list)\n",
    "    plt.yticks(tick_marks, dev_list)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "973e65f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda:0 device.\n",
      "using 86137 images for training, 21515 images for validation.\n",
      "train loss:100%[**************************************************->]0.860\n",
      "[epoch 1] train_loss: 1.049 train_accuracy: 0.651 val_accuracy: 0.759  recall: 0.598  f1: 0.600\n",
      "train loss:100%[**************************************************->]0.441\n",
      "[epoch 2] train_loss: 0.528 train_accuracy: 0.804 val_accuracy: 0.836  recall: 0.712  f1: 0.723\n",
      "train loss:100%[**************************************************->]0.320\n",
      "[epoch 3] train_loss: 0.395 train_accuracy: 0.852 val_accuracy: 0.853  recall: 0.769  f1: 0.772\n",
      "train loss:100%[**************************************************->]0.225\n",
      "[epoch 4] train_loss: 0.317 train_accuracy: 0.883 val_accuracy: 0.883  recall: 0.794  f1: 0.796\n",
      "train loss:100%[**************************************************->]0.107\n",
      "[epoch 5] train_loss: 0.259 train_accuracy: 0.904 val_accuracy: 0.900  recall: 0.811  f1: 0.840\n",
      "train loss:100%[**************************************************->]0.226\n",
      "[epoch 6] train_loss: 0.220 train_accuracy: 0.918 val_accuracy: 0.887  recall: 0.795  f1: 0.819\n",
      "train loss:100%[**************************************************->]0.153\n",
      "[epoch 7] train_loss: 0.192 train_accuracy: 0.929 val_accuracy: 0.911  recall: 0.808  f1: 0.841\n",
      "train loss:100%[**************************************************->]0.045\n",
      "[epoch 8] train_loss: 0.167 train_accuracy: 0.939 val_accuracy: 0.900  recall: 0.777  f1: 0.810\n",
      "train loss:100%[**************************************************->]0.233\n",
      "[epoch 9] train_loss: 0.151 train_accuracy: 0.944 val_accuracy: 0.853  recall: 0.690  f1: 0.718\n",
      "train loss:100%[**************************************************->]0.220\n",
      "[epoch 10] train_loss: 0.137 train_accuracy: 0.949 val_accuracy: 0.923  recall: 0.841  f1: 0.863\n",
      "train loss:100%[**************************************************->]0.139\n",
      "[epoch 11] train_loss: 0.125 train_accuracy: 0.954 val_accuracy: 0.931  recall: 0.875  f1: 0.872\n",
      "train loss:100%[**************************************************->]0.065\n",
      "[epoch 12] train_loss: 0.116 train_accuracy: 0.957 val_accuracy: 0.911  recall: 0.800  f1: 0.817\n",
      "train loss:100%[**************************************************->]0.174\n",
      "[epoch 13] train_loss: 0.107 train_accuracy: 0.960 val_accuracy: 0.929  recall: 0.853  f1: 0.862\n",
      "train loss:100%[**************************************************->]0.086\n",
      "[epoch 14] train_loss: 0.098 train_accuracy: 0.963 val_accuracy: 0.927  recall: 0.862  f1: 0.858\n",
      "train loss:100%[**************************************************->]0.031\n",
      "[epoch 15] train_loss: 0.093 train_accuracy: 0.965 val_accuracy: 0.937  recall: 0.865  f1: 0.888\n",
      "train loss:100%[**************************************************->]0.112\n",
      "[epoch 16] train_loss: 0.090 train_accuracy: 0.966 val_accuracy: 0.342  recall: 0.216  f1: 0.224\n",
      "train loss:100%[**************************************************->]0.016\n",
      "[epoch 17] train_loss: 0.082 train_accuracy: 0.968 val_accuracy: 0.940  recall: 0.865  f1: 0.883\n",
      "train loss:100%[**************************************************->]0.031\n",
      "[epoch 18] train_loss: 0.080 train_accuracy: 0.970 val_accuracy: 0.945  recall: 0.869  f1: 0.874\n",
      "train loss:100%[**************************************************->]0.080\n",
      "[epoch 19] train_loss: 0.079 train_accuracy: 0.971 val_accuracy: 0.940  recall: 0.876  f1: 0.886\n",
      "train loss:100%[**************************************************->]0.096\n",
      "[epoch 20] train_loss: 0.074 train_accuracy: 0.972 val_accuracy: 0.939  recall: 0.871  f1: 0.887\n",
      "train loss:100%[**************************************************->]0.009\n",
      "[epoch 21] train_loss: 0.073 train_accuracy: 0.972 val_accuracy: 0.934  recall: 0.844  f1: 0.866\n",
      "train loss:100%[**************************************************->]0.306\n",
      "[epoch 22] train_loss: 0.068 train_accuracy: 0.974 val_accuracy: 0.922  recall: 0.812  f1: 0.835\n",
      "train loss:100%[**************************************************->]0.248\n",
      "[epoch 23] train_loss: 0.069 train_accuracy: 0.974 val_accuracy: 0.939  recall: 0.876  f1: 0.885\n",
      "train loss:100%[**************************************************->]0.002\n",
      "[epoch 24] train_loss: 0.065 train_accuracy: 0.975 val_accuracy: 0.942  recall: 0.884  f1: 0.894\n",
      "train loss:100%[**************************************************->]0.072\n",
      "[epoch 25] train_loss: 0.063 train_accuracy: 0.975 val_accuracy: 0.952  recall: 0.892  f1: 0.895\n",
      "train loss:100%[**************************************************->]0.181\n",
      "[epoch 26] train_loss: 0.063 train_accuracy: 0.976 val_accuracy: 0.946  recall: 0.892  f1: 0.889\n",
      "train loss:100%[**************************************************->]0.269\n",
      "[epoch 27] train_loss: 0.058 train_accuracy: 0.978 val_accuracy: 0.927  recall: 0.876  f1: 0.864\n",
      "train loss:100%[**************************************************->]0.219\n",
      "[epoch 28] train_loss: 0.058 train_accuracy: 0.978 val_accuracy: 0.951  recall: 0.894  f1: 0.896\n",
      "train loss:100%[**************************************************->]0.058\n",
      "[epoch 29] train_loss: 0.058 train_accuracy: 0.977 val_accuracy: 0.920  recall: 0.852  f1: 0.841\n",
      "train loss:100%[**************************************************->]0.116\n",
      "[epoch 30] train_loss: 0.056 train_accuracy: 0.979 val_accuracy: 0.950  recall: 0.900  f1: 0.899\n",
      "train loss:100%[**************************************************->]0.058\n",
      "[epoch 31] train_loss: 0.055 train_accuracy: 0.979 val_accuracy: 0.945  recall: 0.882  f1: 0.878\n",
      "train loss:100%[**************************************************->]0.015\n",
      "[epoch 32] train_loss: 0.055 train_accuracy: 0.979 val_accuracy: 0.950  recall: 0.898  f1: 0.904\n",
      "train loss:100%[**************************************************->]0.084\n",
      "[epoch 33] train_loss: 0.053 train_accuracy: 0.979 val_accuracy: 0.918  recall: 0.864  f1: 0.866\n",
      "train loss:100%[**************************************************->]0.033\n",
      "[epoch 34] train_loss: 0.051 train_accuracy: 0.980 val_accuracy: 0.948  recall: 0.887  f1: 0.896\n",
      "train loss:100%[**************************************************->]0.147\n",
      "[epoch 35] train_loss: 0.051 train_accuracy: 0.981 val_accuracy: 0.941  recall: 0.886  f1: 0.884\n",
      "train loss:100%[**************************************************->]0.128\n",
      "[epoch 36] train_loss: 0.049 train_accuracy: 0.981 val_accuracy: 0.952  recall: 0.897  f1: 0.894\n",
      "train loss:100%[**************************************************->]0.020\n",
      "[epoch 37] train_loss: 0.050 train_accuracy: 0.981 val_accuracy: 0.956  recall: 0.906  f1: 0.918\n",
      "train loss:100%[**************************************************->]0.015\n",
      "[epoch 38] train_loss: 0.048 train_accuracy: 0.981 val_accuracy: 0.950  recall: 0.899  f1: 0.895\n",
      "train loss:100%[**************************************************->]0.111\n",
      "[epoch 39] train_loss: 0.049 train_accuracy: 0.981 val_accuracy: 0.954  recall: 0.893  f1: 0.899\n",
      "train loss:100%[**************************************************->]0.057\n",
      "[epoch 40] train_loss: 0.048 train_accuracy: 0.981 val_accuracy: 0.934  recall: 0.875  f1: 0.887\n",
      "train loss:100%[**************************************************->]0.019\n",
      "[epoch 41] train_loss: 0.046 train_accuracy: 0.982 val_accuracy: 0.953  recall: 0.893  f1: 0.900\n",
      "train loss:100%[**************************************************->]0.073\n",
      "[epoch 42] train_loss: 0.048 train_accuracy: 0.982 val_accuracy: 0.956  recall: 0.898  f1: 0.901\n",
      "train loss:100%[**************************************************->]0.003\n",
      "[epoch 43] train_loss: 0.045 train_accuracy: 0.983 val_accuracy: 0.948  recall: 0.887  f1: 0.898\n",
      "train loss:100%[**************************************************->]0.049\n",
      "[epoch 44] train_loss: 0.044 train_accuracy: 0.983 val_accuracy: 0.951  recall: 0.901  f1: 0.905\n",
      "train loss:100%[**************************************************->]0.047\n",
      "[epoch 45] train_loss: 0.045 train_accuracy: 0.983 val_accuracy: 0.949  recall: 0.880  f1: 0.884\n",
      "train loss:100%[**************************************************->]0.000\n",
      "[epoch 46] train_loss: 0.043 train_accuracy: 0.984 val_accuracy: 0.960  recall: 0.912  f1: 0.911\n",
      "train loss:100%[**************************************************->]0.002\n",
      "[epoch 47] train_loss: 0.045 train_accuracy: 0.983 val_accuracy: 0.957  recall: 0.909  f1: 0.912\n",
      "train loss:100%[**************************************************->]0.143\n",
      "[epoch 48] train_loss: 0.042 train_accuracy: 0.984 val_accuracy: 0.947  recall: 0.880  f1: 0.886\n",
      "train loss:100%[**************************************************->]0.002\n",
      "[epoch 49] train_loss: 0.044 train_accuracy: 0.983 val_accuracy: 0.959  recall: 0.905  f1: 0.904\n",
      "train loss:100%[**************************************************->]0.003\n",
      "[epoch 50] train_loss: 0.041 train_accuracy: 0.984 val_accuracy: 0.953  recall: 0.897  f1: 0.905\n",
      "train loss:100%[**************************************************->]0.012\n",
      "[epoch 51] train_loss: 0.042 train_accuracy: 0.984 val_accuracy: 0.957  recall: 0.907  f1: 0.914\n",
      "train loss:100%[**************************************************->]0.002\n",
      "[epoch 52] train_loss: 0.040 train_accuracy: 0.984 val_accuracy: 0.958  recall: 0.900  f1: 0.903\n",
      "train loss:100%[**************************************************->]0.110\n",
      "[epoch 53] train_loss: 0.043 train_accuracy: 0.983 val_accuracy: 0.959  recall: 0.899  f1: 0.894\n",
      "train loss:100%[**************************************************->]0.003\n",
      "[epoch 54] train_loss: 0.040 train_accuracy: 0.985 val_accuracy: 0.958  recall: 0.899  f1: 0.904\n",
      "train loss:100%[**************************************************->]0.037\n",
      "[epoch 55] train_loss: 0.040 train_accuracy: 0.984 val_accuracy: 0.959  recall: 0.909  f1: 0.915\n",
      "train loss:100%[**************************************************->]0.028\n",
      "[epoch 56] train_loss: 0.040 train_accuracy: 0.984 val_accuracy: 0.955  recall: 0.904  f1: 0.904\n",
      "train loss:100%[**************************************************->]0.124\n",
      "[epoch 57] train_loss: 0.039 train_accuracy: 0.984 val_accuracy: 0.954  recall: 0.892  f1: 0.897\n",
      "train loss:100%[**************************************************->]0.044\n",
      "[epoch 58] train_loss: 0.037 train_accuracy: 0.985 val_accuracy: 0.956  recall: 0.900  f1: 0.911\n",
      "train loss:100%[**************************************************->]0.090\n",
      "[epoch 59] train_loss: 0.040 train_accuracy: 0.984 val_accuracy: 0.962  recall: 0.912  f1: 0.916\n",
      "train loss:100%[**************************************************->]0.005\n",
      "[epoch 60] train_loss: 0.038 train_accuracy: 0.985 val_accuracy: 0.955  recall: 0.896  f1: 0.899\n",
      "train loss:100%[**************************************************->]0.001\n",
      "[epoch 61] train_loss: 0.038 train_accuracy: 0.985 val_accuracy: 0.958  recall: 0.911  f1: 0.912\n",
      "train loss:100%[**************************************************->]0.127\n",
      "[epoch 62] train_loss: 0.038 train_accuracy: 0.985 val_accuracy: 0.959  recall: 0.901  f1: 0.900\n",
      "train loss:100%[**************************************************->]0.001\n",
      "[epoch 63] train_loss: 0.037 train_accuracy: 0.986 val_accuracy: 0.958  recall: 0.904  f1: 0.909\n",
      "train loss:100%[**************************************************->]0.019\n",
      "[epoch 64] train_loss: 0.036 train_accuracy: 0.986 val_accuracy: 0.959  recall: 0.899  f1: 0.899\n",
      "train loss:100%[**************************************************->]0.090\n",
      "[epoch 65] train_loss: 0.036 train_accuracy: 0.986 val_accuracy: 0.960  recall: 0.904  f1: 0.905\n",
      "train loss:100%[**************************************************->]0.000\n",
      "[epoch 66] train_loss: 0.039 train_accuracy: 0.985 val_accuracy: 0.953  recall: 0.900  f1: 0.904\n",
      "train loss:100%[**************************************************->]0.000\n",
      "[epoch 67] train_loss: 0.035 train_accuracy: 0.986 val_accuracy: 0.955  recall: 0.906  f1: 0.913\n",
      "train loss:100%[**************************************************->]0.030\n",
      "[epoch 68] train_loss: 0.037 train_accuracy: 0.985 val_accuracy: 0.958  recall: 0.911  f1: 0.912\n",
      "train loss:100%[**************************************************->]0.014\n",
      "[epoch 69] train_loss: 0.035 train_accuracy: 0.986 val_accuracy: 0.958  recall: 0.905  f1: 0.913\n",
      "train loss:100%[**************************************************->]0.001\n",
      "[epoch 70] train_loss: 0.037 train_accuracy: 0.986 val_accuracy: 0.961  recall: 0.914  f1: 0.913\n",
      "train loss:100%[**************************************************->]0.017\n",
      "[epoch 71] train_loss: 0.035 train_accuracy: 0.986 val_accuracy: 0.958  recall: 0.904  f1: 0.906\n",
      "train loss:100%[**************************************************->]0.004\n",
      "[epoch 72] train_loss: 0.035 train_accuracy: 0.986 val_accuracy: 0.942  recall: 0.856  f1: 0.875\n",
      "train loss:100%[**************************************************->]0.012\n",
      "[epoch 73] train_loss: 0.035 train_accuracy: 0.986 val_accuracy: 0.959  recall: 0.911  f1: 0.905\n",
      "train loss:100%[**************************************************->]0.006\n",
      "[epoch 74] train_loss: 0.035 train_accuracy: 0.986 val_accuracy: 0.960  recall: 0.905  f1: 0.913\n",
      "train loss:100%[**************************************************->]0.001\n",
      "[epoch 75] train_loss: 0.034 train_accuracy: 0.987 val_accuracy: 0.957  recall: 0.901  f1: 0.900\n",
      "train loss:100%[**************************************************->]0.001\n",
      "[epoch 76] train_loss: 0.035 train_accuracy: 0.986 val_accuracy: 0.952  recall: 0.892  f1: 0.903\n",
      "train loss:100%[**************************************************->]0.008\n",
      "[epoch 77] train_loss: 0.034 train_accuracy: 0.987 val_accuracy: 0.960  recall: 0.901  f1: 0.907\n",
      "train loss:100%[**************************************************->]0.003\n",
      "[epoch 78] train_loss: 0.034 train_accuracy: 0.987 val_accuracy: 0.958  recall: 0.903  f1: 0.906\n",
      "train loss:100%[**************************************************->]0.000\n",
      "[epoch 79] train_loss: 0.033 train_accuracy: 0.987 val_accuracy: 0.962  recall: 0.908  f1: 0.912\n",
      "train loss:100%[**************************************************->]0.052\n",
      "[epoch 80] train_loss: 0.035 train_accuracy: 0.986 val_accuracy: 0.964  recall: 0.909  f1: 0.912\n",
      "train loss:100%[**************************************************->]0.014\n",
      "[epoch 81] train_loss: 0.032 train_accuracy: 0.987 val_accuracy: 0.963  recall: 0.916  f1: 0.924\n",
      "train loss:100%[**************************************************->]0.005\n",
      "[epoch 82] train_loss: 0.033 train_accuracy: 0.987 val_accuracy: 0.961  recall: 0.917  f1: 0.911\n",
      "train loss:100%[**************************************************->]0.004\n",
      "[epoch 83] train_loss: 0.032 train_accuracy: 0.987 val_accuracy: 0.955  recall: 0.892  f1: 0.906\n",
      "train loss:100%[**************************************************->]0.036\n",
      "[epoch 84] train_loss: 0.033 train_accuracy: 0.987 val_accuracy: 0.961  recall: 0.911  f1: 0.913\n",
      "train loss:100%[**************************************************->]0.001\n",
      "[epoch 85] train_loss: 0.032 train_accuracy: 0.987 val_accuracy: 0.964  recall: 0.909  f1: 0.914\n",
      "train loss:100%[**************************************************->]0.000\n",
      "[epoch 86] train_loss: 0.034 train_accuracy: 0.987 val_accuracy: 0.959  recall: 0.911  f1: 0.912\n",
      "train loss:100%[**************************************************->]0.006\n",
      "[epoch 87] train_loss: 0.033 train_accuracy: 0.987 val_accuracy: 0.962  recall: 0.913  f1: 0.913\n",
      "train loss:100%[**************************************************->]0.009\n",
      "[epoch 88] train_loss: 0.032 train_accuracy: 0.987 val_accuracy: 0.960  recall: 0.902  f1: 0.909\n",
      "train loss:100%[**************************************************->]0.056\n",
      "[epoch 89] train_loss: 0.032 train_accuracy: 0.987 val_accuracy: 0.963  recall: 0.920  f1: 0.919\n",
      "train loss:100%[**************************************************->]0.023\n",
      "[epoch 90] train_loss: 0.032 train_accuracy: 0.987 val_accuracy: 0.959  recall: 0.901  f1: 0.912\n",
      "train loss:100%[**************************************************->]0.003\n",
      "[epoch 91] train_loss: 0.032 train_accuracy: 0.987 val_accuracy: 0.961  recall: 0.917  f1: 0.917\n",
      "train loss:100%[**************************************************->]0.001\n",
      "[epoch 92] train_loss: 0.030 train_accuracy: 0.988 val_accuracy: 0.960  recall: 0.910  f1: 0.907\n",
      "train loss:100%[**************************************************->]0.003\n",
      "[epoch 93] train_loss: 0.031 train_accuracy: 0.988 val_accuracy: 0.960  recall: 0.899  f1: 0.902\n",
      "train loss:100%[**************************************************->]0.045\n",
      "[epoch 94] train_loss: 0.031 train_accuracy: 0.987 val_accuracy: 0.961  recall: 0.913  f1: 0.909\n",
      "train loss:100%[**************************************************->]0.115\n",
      "[epoch 95] train_loss: 0.030 train_accuracy: 0.988 val_accuracy: 0.960  recall: 0.906  f1: 0.905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:100%[**************************************************->]0.000\n",
      "[epoch 96] train_loss: 0.031 train_accuracy: 0.988 val_accuracy: 0.963  recall: 0.919  f1: 0.919\n",
      "train loss:100%[**************************************************->]0.004\n",
      "[epoch 97] train_loss: 0.031 train_accuracy: 0.987 val_accuracy: 0.963  recall: 0.911  f1: 0.912\n",
      "train loss:100%[**************************************************->]0.001\n",
      "[epoch 98] train_loss: 0.031 train_accuracy: 0.987 val_accuracy: 0.956  recall: 0.900  f1: 0.886\n",
      "train loss:100%[**************************************************->]0.030\n",
      "[epoch 99] train_loss: 0.032 train_accuracy: 0.988 val_accuracy: 0.964  recall: 0.917  f1: 0.919\n",
      "train loss:100%[**************************************************->]0.063\n",
      "[epoch 100] train_loss: 0.029 train_accuracy: 0.988 val_accuracy: 0.964  recall: 0.919  f1: 0.917\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.categories = sorted(os.listdir(data_dir))\n",
    "        self.data = []\n",
    "        self.transform = transform\n",
    "        for category in self.categories:\n",
    "            category_dir = os.path.join(data_dir, category)\n",
    "            category_data = sorted(os.listdir(category_dir))\n",
    "            self.data.extend([(os.path.join(category_dir, file), self.categories.index(category)) for file in category_data])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_path, label = self.data[index]\n",
    "        data = np.load(file_path)\n",
    "        image = Image.fromarray(data.astype(np.uint8))\n",
    "        image = transform(image)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using {} device.\".format(device))\n",
    "\n",
    "transform = transforms.Compose([ transforms.Grayscale(num_output_channels = 1),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5), (0.5))])\n",
    "\n",
    "train_dataset = CustomDataset(\"features/train_npy\",transform=transform)\n",
    "\n",
    "train_num = len(train_dataset)\n",
    "\n",
    "dev_list = train_dataset.categories\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size, shuffle=True,\n",
    "                                               num_workers=0)\n",
    "\n",
    "validate_dataset = CustomDataset(\"features/val_npy\",transform=transform)\n",
    "val_num = len(validate_dataset)\n",
    "validate_loader = torch.utils.data.DataLoader(validate_dataset,\n",
    "                                                  batch_size=batch_size, shuffle=False,\n",
    "                                                  num_workers=0)\n",
    "\n",
    "print(\"using {} images for training, {} images for validation.\".format(train_num, val_num))\n",
    "\n",
    "net = se_resnet34(num_classes=31)\n",
    "net.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "epochs = 100\n",
    "save_path = './TrimSENet_parameters.pth'\n",
    "best_f1 = 0.0\n",
    "train_accurate_list = []\n",
    "val_accurate_list = []\n",
    "f1_list = []\n",
    "recall_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    for step, data in enumerate(train_loader, start=0):\n",
    "        images, labels = data\n",
    "        images = images.reshape(images.shape[0], 1, 1500)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images.to(device))\n",
    "        predict_y = torch.max(outputs, dim=1)[1]\n",
    "        train_acc += torch.eq(predict_y, labels.to(device)).sum().item()\n",
    "        loss = loss_function(outputs,labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        rate = (step + 1) / len(train_loader)\n",
    "        a = \"*\" * int(rate * 50)\n",
    "        b = \".\" * int((1 - rate) * 50)\n",
    "        print(\"\\rtrain loss:{:^3.0f}%[{}->{}]{:.3f}\".format(int(rate * 100), a, b, loss), end=\"\")\n",
    "    print()\n",
    "    train_accurate = train_acc / train_num\n",
    "    train_accurate_list.append(train_accurate)\n",
    "    net.eval()\n",
    "    acc = 0.0  \n",
    "    val = torch.tensor([])\n",
    "    pre = torch.tensor([])\n",
    "    with torch.no_grad():\n",
    "        for val_data in validate_loader:\n",
    "            val_images, val_labels = val_data\n",
    "            val_images = val_images.reshape(val_images.shape[0], 1, 1500)\n",
    "            outputs = net(val_images.to(device))\n",
    "            predict_y = torch.max(outputs, dim=1)[1]\n",
    "            pre = torch.cat([pre.to(device), predict_y.to(device)])\n",
    "            val = torch.cat([val.to(device), val_labels.to(device)])\n",
    "            acc += torch.eq(predict_y, val_labels.to(device)).sum().item()\n",
    "    val_accurate = acc / val_num\n",
    "    val_accurate_list.append(val_accurate)\n",
    "    f1 = f1_score(val.cpu(), pre.cpu(), average='macro')\n",
    "    recall = recall_score(val.cpu(), pre.cpu(), average='macro')\n",
    "\n",
    "    f1_list.append(f1)\n",
    "    recall_list.append(recall)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_pre = pre\n",
    "        best_val = val\n",
    "        torch.save(net.state_dict(), save_path)\n",
    "        torch.save(best_pre, 'pre_val_label/best_pre_TrimSENet.pt')\n",
    "        torch.save(best_val, 'pre_val_label/best_val_TrimSENet.pt')\n",
    "    print('[epoch %d] train_loss: %.3f train_accuracy: %.3f val_accuracy: %.3f  recall: %.3f  f1: %.3f' %\n",
    "              (epoch + 1, running_loss / step, train_accurate, val_accurate, recall, f1))\n",
    "    with open(\"TrimSENet_result_npy.txt\", 'a') as file:\n",
    "        file.write(\"[epoch \" + str(epoch + 1) + \"]\" + \"  \" + \"train_accuracy:\" + str(train_accurate) + \"  \" + \"val_accuracy:\" + str(val_accurate) + \"  \" + \"recall:\" + str(recall) + \"  \" + \"f1:\" + str(f1) + '\\n')\n",
    "print('Finished Training')\n",
    "iterations = range(1, len(train_accurate_list) + 1)\n",
    "with open(\"TrimSENet_npy_plt_data.txt\", 'a') as file:\n",
    "    file.write(\"iterations:\" + str(iterations) +\n",
    "               \"train_accurate_list:\" + str(train_accurate_list) +\n",
    "               \"val_accurate_list:\" + str(val_accurate_list) +\n",
    "               \"f1_list:\" + str(f1_list) +\n",
    "               \"recall_list:\" + str(recall_list) +\n",
    "               \"dev_list:\" + str(dev_list) + '\\n')\n",
    "conf_matrix = confusion_matrix(best_val.cpu(),best_pre.cpu())\n",
    "plot_matrix(conf_matrix,dev_list,\"TrimSENet_confusion_matrix_npy.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a90150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
