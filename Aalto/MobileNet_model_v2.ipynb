{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0440018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "\n",
    "def _make_divisible(ch, divisor=8, min_ch=None):\n",
    "    if min_ch is None:\n",
    "        min_ch = divisor\n",
    "    new_ch = max(min_ch, int(ch + divisor / 2) // divisor * divisor)\n",
    "    if new_ch < 0.9 * ch:\n",
    "        new_ch += divisor\n",
    "    return new_ch\n",
    "\n",
    "\n",
    "class ConvBNReLU(nn.Sequential):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size=3, stride=1, groups=1):\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        super(ConvBNReLU, self).__init__(\n",
    "            nn.Conv1d(in_channel, out_channel, kernel_size, stride, padding, groups=groups, bias=False),\n",
    "            nn.BatchNorm1d(out_channel),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        hidden_channel = in_channel * expand_ratio\n",
    "        self.use_shortcut = stride == 1 and in_channel == out_channel\n",
    "\n",
    "        layers = []\n",
    "        if expand_ratio != 1:\n",
    "            layers.append(ConvBNReLU(in_channel, hidden_channel, kernel_size=1))\n",
    "        layers.extend([\n",
    "            ConvBNReLU(hidden_channel, hidden_channel, stride=stride, groups=hidden_channel),\n",
    "            nn.Conv1d(hidden_channel, out_channel, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm1d(out_channel),\n",
    "        ])\n",
    "\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_shortcut:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes=40, alpha=1.0, round_nearest=8):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = _make_divisible(32 * alpha, round_nearest)\n",
    "        last_channel = _make_divisible(1280 * alpha, round_nearest)\n",
    "\n",
    "        inverted_residual_setting = [\n",
    "            # t, c, n, s\n",
    "            [1, 16, 1, 1],\n",
    "            [6, 24, 2, 2],\n",
    "            [6, 32, 3, 2],\n",
    "            [6, 64, 4, 2],\n",
    "            [6, 96, 3, 1],\n",
    "            [6, 160, 3, 2],\n",
    "            [6, 320, 1, 1],\n",
    "        ]\n",
    "\n",
    "        features = []\n",
    "        features.append(ConvBNReLU(1, input_channel, stride=2))\n",
    "        for t, c, n, s in inverted_residual_setting:\n",
    "            output_channel = _make_divisible(c * alpha, round_nearest)\n",
    "            for i in range(n):\n",
    "                stride = s if i == 0 else 1\n",
    "                features.append(block(input_channel, output_channel, stride, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "        features.append(ConvBNReLU(input_channel, last_channel, 1))\n",
    "        self.features = nn.Sequential(*features)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(last_channel, num_classes)\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00fc2b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_matrix(conf_matrix, dev_list, save_path):\n",
    "    plt.figure(figsize=(20, 16), dpi=300)\n",
    "    plt.imshow(conf_matrix, cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "\n",
    "    thresh = conf_matrix.max() / 2.\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        for j in range(conf_matrix.shape[1]):\n",
    "            plt.text(j, i, conf_matrix[i, j],\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if conf_matrix[i, j] > thresh else \"black\", fontsize=6)\n",
    "\n",
    "    tick_marks = np.arange(len(dev_list))\n",
    "    plt.xticks(tick_marks, dev_list)\n",
    "    plt.yticks(tick_marks, dev_list)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f46352c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda:0 device.\n",
      "using 86137 images for training, 21515 images for validation.\n",
      "train loss:100%[**************************************************->]1.329\n",
      "[epoch 1] train_loss: 1.744 train_accuracy: 0.443 val_accuracy: 0.133  recall: 0.067  f1: 0.045\n",
      "train loss:100%[**************************************************->]0.985\n",
      "[epoch 2] train_loss: 1.086 train_accuracy: 0.609 val_accuracy: 0.581  recall: 0.371  f1: 0.327\n",
      "train loss:100%[**************************************************->]0.408\n",
      "[epoch 3] train_loss: 0.850 train_accuracy: 0.682 val_accuracy: 0.649  recall: 0.452  f1: 0.434\n",
      "train loss:100%[**************************************************->]0.739\n",
      "[epoch 4] train_loss: 0.722 train_accuracy: 0.726 val_accuracy: 0.734  recall: 0.588  f1: 0.572\n",
      "train loss:100%[**************************************************->]0.550\n",
      "[epoch 5] train_loss: 0.637 train_accuracy: 0.760 val_accuracy: 0.405  recall: 0.349  f1: 0.328\n",
      "train loss:100%[**************************************************->]0.530\n",
      "[epoch 6] train_loss: 0.571 train_accuracy: 0.783 val_accuracy: 0.707  recall: 0.539  f1: 0.552\n",
      "train loss:100%[**************************************************->]0.359\n",
      "[epoch 7] train_loss: 0.511 train_accuracy: 0.805 val_accuracy: 0.805  recall: 0.708  f1: 0.714\n",
      "train loss:100%[**************************************************->]0.281\n",
      "[epoch 8] train_loss: 0.468 train_accuracy: 0.821 val_accuracy: 0.818  recall: 0.725  f1: 0.744\n",
      "train loss:100%[**************************************************->]0.523\n",
      "[epoch 9] train_loss: 0.434 train_accuracy: 0.835 val_accuracy: 0.837  recall: 0.760  f1: 0.759\n",
      "train loss:100%[**************************************************->]1.012\n",
      "[epoch 10] train_loss: 0.399 train_accuracy: 0.849 val_accuracy: 0.776  recall: 0.693  f1: 0.697\n",
      "train loss:100%[**************************************************->]0.510\n",
      "[epoch 11] train_loss: 0.371 train_accuracy: 0.860 val_accuracy: 0.759  recall: 0.684  f1: 0.619\n",
      "train loss:100%[**************************************************->]0.398\n",
      "[epoch 12] train_loss: 0.345 train_accuracy: 0.868 val_accuracy: 0.806  recall: 0.744  f1: 0.732\n",
      "train loss:100%[**************************************************->]0.699\n",
      "[epoch 13] train_loss: 0.328 train_accuracy: 0.877 val_accuracy: 0.845  recall: 0.746  f1: 0.757\n",
      "train loss:100%[**************************************************->]0.366\n",
      "[epoch 14] train_loss: 0.307 train_accuracy: 0.885 val_accuracy: 0.781  recall: 0.679  f1: 0.659\n",
      "train loss:100%[**************************************************->]0.400\n",
      "[epoch 15] train_loss: 0.285 train_accuracy: 0.894 val_accuracy: 0.746  recall: 0.687  f1: 0.650\n",
      "train loss:100%[**************************************************->]0.200\n",
      "[epoch 16] train_loss: 0.270 train_accuracy: 0.898 val_accuracy: 0.752  recall: 0.704  f1: 0.692\n",
      "train loss:100%[**************************************************->]0.278\n",
      "[epoch 17] train_loss: 0.256 train_accuracy: 0.906 val_accuracy: 0.775  recall: 0.688  f1: 0.671\n",
      "train loss:100%[**************************************************->]0.378\n",
      "[epoch 18] train_loss: 0.244 train_accuracy: 0.909 val_accuracy: 0.893  recall: 0.818  f1: 0.843\n",
      "train loss:100%[**************************************************->]0.150\n",
      "[epoch 19] train_loss: 0.230 train_accuracy: 0.914 val_accuracy: 0.795  recall: 0.684  f1: 0.711\n",
      "train loss:100%[**************************************************->]0.179\n",
      "[epoch 20] train_loss: 0.225 train_accuracy: 0.917 val_accuracy: 0.672  recall: 0.651  f1: 0.584\n",
      "train loss:100%[**************************************************->]0.333\n",
      "[epoch 21] train_loss: 0.216 train_accuracy: 0.920 val_accuracy: 0.908  recall: 0.831  f1: 0.852\n",
      "train loss:100%[**************************************************->]0.180\n",
      "[epoch 22] train_loss: 0.201 train_accuracy: 0.925 val_accuracy: 0.880  recall: 0.817  f1: 0.824\n",
      "train loss:100%[**************************************************->]0.009\n",
      "[epoch 23] train_loss: 0.197 train_accuracy: 0.926 val_accuracy: 0.900  recall: 0.831  f1: 0.826\n",
      "train loss:100%[**************************************************->]0.247\n",
      "[epoch 24] train_loss: 0.189 train_accuracy: 0.929 val_accuracy: 0.915  recall: 0.852  f1: 0.863\n",
      "train loss:100%[**************************************************->]0.144\n",
      "[epoch 25] train_loss: 0.184 train_accuracy: 0.933 val_accuracy: 0.839  recall: 0.717  f1: 0.707\n",
      "train loss:100%[**************************************************->]0.266\n",
      "[epoch 26] train_loss: 0.173 train_accuracy: 0.936 val_accuracy: 0.891  recall: 0.824  f1: 0.817\n",
      "train loss:100%[**************************************************->]0.115\n",
      "[epoch 27] train_loss: 0.170 train_accuracy: 0.938 val_accuracy: 0.688  recall: 0.534  f1: 0.498\n",
      "train loss:100%[**************************************************->]0.080\n",
      "[epoch 28] train_loss: 0.164 train_accuracy: 0.938 val_accuracy: 0.900  recall: 0.839  f1: 0.832\n",
      "train loss:100%[**************************************************->]0.276\n",
      "[epoch 29] train_loss: 0.158 train_accuracy: 0.941 val_accuracy: 0.884  recall: 0.816  f1: 0.812\n",
      "train loss:100%[**************************************************->]0.365\n",
      "[epoch 30] train_loss: 0.155 train_accuracy: 0.943 val_accuracy: 0.830  recall: 0.771  f1: 0.755\n",
      "train loss:100%[**************************************************->]0.214\n",
      "[epoch 31] train_loss: 0.152 train_accuracy: 0.943 val_accuracy: 0.829  recall: 0.683  f1: 0.659\n",
      "train loss:100%[**************************************************->]0.134\n",
      "[epoch 32] train_loss: 0.142 train_accuracy: 0.947 val_accuracy: 0.865  recall: 0.773  f1: 0.755\n",
      "train loss:100%[**************************************************->]0.042\n",
      "[epoch 33] train_loss: 0.144 train_accuracy: 0.947 val_accuracy: 0.800  recall: 0.554  f1: 0.568\n",
      "train loss:100%[**************************************************->]0.079\n",
      "[epoch 34] train_loss: 0.135 train_accuracy: 0.950 val_accuracy: 0.548  recall: 0.375  f1: 0.366\n",
      "train loss:100%[**************************************************->]0.133\n",
      "[epoch 35] train_loss: 0.133 train_accuracy: 0.951 val_accuracy: 0.649  recall: 0.468  f1: 0.512\n",
      "train loss:100%[**************************************************->]0.104\n",
      "[epoch 36] train_loss: 0.127 train_accuracy: 0.954 val_accuracy: 0.929  recall: 0.858  f1: 0.883\n",
      "train loss:100%[**************************************************->]0.008\n",
      "[epoch 37] train_loss: 0.127 train_accuracy: 0.953 val_accuracy: 0.911  recall: 0.822  f1: 0.834\n",
      "train loss:100%[**************************************************->]0.184\n",
      "[epoch 38] train_loss: 0.125 train_accuracy: 0.953 val_accuracy: 0.932  recall: 0.866  f1: 0.883\n",
      "train loss:100%[**************************************************->]0.029\n",
      "[epoch 39] train_loss: 0.119 train_accuracy: 0.956 val_accuracy: 0.714  recall: 0.495  f1: 0.436\n",
      "train loss:100%[**************************************************->]0.003\n",
      "[epoch 40] train_loss: 0.115 train_accuracy: 0.957 val_accuracy: 0.159  recall: 0.142  f1: 0.156\n",
      "train loss:100%[**************************************************->]0.144\n",
      "[epoch 41] train_loss: 0.113 train_accuracy: 0.957 val_accuracy: 0.888  recall: 0.827  f1: 0.816\n",
      "train loss:100%[**************************************************->]0.085\n",
      "[epoch 42] train_loss: 0.111 train_accuracy: 0.959 val_accuracy: 0.933  recall: 0.879  f1: 0.878\n",
      "train loss:100%[**************************************************->]0.066\n",
      "[epoch 43] train_loss: 0.106 train_accuracy: 0.960 val_accuracy: 0.935  recall: 0.876  f1: 0.882\n",
      "train loss:100%[**************************************************->]0.090\n",
      "[epoch 44] train_loss: 0.105 train_accuracy: 0.962 val_accuracy: 0.804  recall: 0.725  f1: 0.725\n",
      "train loss:100%[**************************************************->]0.118\n",
      "[epoch 45] train_loss: 0.106 train_accuracy: 0.961 val_accuracy: 0.860  recall: 0.811  f1: 0.786\n",
      "train loss:100%[**************************************************->]0.073\n",
      "[epoch 46] train_loss: 0.100 train_accuracy: 0.962 val_accuracy: 0.719  recall: 0.451  f1: 0.430\n",
      "train loss:100%[**************************************************->]0.025\n",
      "[epoch 47] train_loss: 0.098 train_accuracy: 0.964 val_accuracy: 0.941  recall: 0.877  f1: 0.882\n",
      "train loss:100%[**************************************************->]0.307\n",
      "[epoch 48] train_loss: 0.100 train_accuracy: 0.962 val_accuracy: 0.914  recall: 0.853  f1: 0.876\n",
      "train loss:100%[**************************************************->]0.119\n",
      "[epoch 49] train_loss: 0.092 train_accuracy: 0.965 val_accuracy: 0.617  recall: 0.522  f1: 0.508\n",
      "train loss:100%[**************************************************->]0.106\n",
      "[epoch 50] train_loss: 0.095 train_accuracy: 0.965 val_accuracy: 0.939  recall: 0.887  f1: 0.891\n",
      "train loss:100%[**************************************************->]0.008\n",
      "[epoch 51] train_loss: 0.090 train_accuracy: 0.966 val_accuracy: 0.935  recall: 0.860  f1: 0.877\n",
      "train loss:100%[**************************************************->]0.181\n",
      "[epoch 52] train_loss: 0.090 train_accuracy: 0.966 val_accuracy: 0.692  recall: 0.601  f1: 0.565\n",
      "train loss:100%[**************************************************->]0.042\n",
      "[epoch 53] train_loss: 0.087 train_accuracy: 0.967 val_accuracy: 0.821  recall: 0.727  f1: 0.756\n",
      "train loss:100%[**************************************************->]0.130\n",
      "[epoch 54] train_loss: 0.085 train_accuracy: 0.968 val_accuracy: 0.610  recall: 0.608  f1: 0.603\n",
      "train loss:100%[**************************************************->]0.091\n",
      "[epoch 55] train_loss: 0.083 train_accuracy: 0.969 val_accuracy: 0.595  recall: 0.432  f1: 0.452\n",
      "train loss:100%[**************************************************->]0.341\n",
      "[epoch 56] train_loss: 0.080 train_accuracy: 0.970 val_accuracy: 0.940  recall: 0.882  f1: 0.871\n",
      "train loss:100%[**************************************************->]0.027\n",
      "[epoch 57] train_loss: 0.084 train_accuracy: 0.968 val_accuracy: 0.929  recall: 0.851  f1: 0.882\n",
      "train loss:100%[**************************************************->]0.153\n",
      "[epoch 58] train_loss: 0.082 train_accuracy: 0.969 val_accuracy: 0.308  recall: 0.257  f1: 0.270\n",
      "train loss:100%[**************************************************->]0.269\n",
      "[epoch 59] train_loss: 0.079 train_accuracy: 0.970 val_accuracy: 0.316  recall: 0.353  f1: 0.353\n",
      "train loss:100%[**************************************************->]0.114\n",
      "[epoch 60] train_loss: 0.081 train_accuracy: 0.970 val_accuracy: 0.943  recall: 0.885  f1: 0.893\n",
      "train loss:100%[**************************************************->]0.086\n",
      "[epoch 61] train_loss: 0.079 train_accuracy: 0.970 val_accuracy: 0.937  recall: 0.872  f1: 0.878\n",
      "train loss:100%[**************************************************->]0.050\n",
      "[epoch 62] train_loss: 0.074 train_accuracy: 0.972 val_accuracy: 0.942  recall: 0.880  f1: 0.895\n",
      "train loss:100%[**************************************************->]0.148\n",
      "[epoch 63] train_loss: 0.075 train_accuracy: 0.971 val_accuracy: 0.949  recall: 0.885  f1: 0.887\n",
      "train loss:100%[**************************************************->]0.285\n",
      "[epoch 64] train_loss: 0.072 train_accuracy: 0.973 val_accuracy: 0.872  recall: 0.754  f1: 0.780\n",
      "train loss:100%[**************************************************->]0.004\n",
      "[epoch 65] train_loss: 0.074 train_accuracy: 0.972 val_accuracy: 0.894  recall: 0.838  f1: 0.802\n",
      "train loss:100%[**************************************************->]0.015\n",
      "[epoch 66] train_loss: 0.072 train_accuracy: 0.973 val_accuracy: 0.949  recall: 0.890  f1: 0.896\n",
      "train loss:100%[**************************************************->]0.061\n",
      "[epoch 67] train_loss: 0.070 train_accuracy: 0.973 val_accuracy: 0.942  recall: 0.886  f1: 0.890\n",
      "train loss:100%[**************************************************->]0.108\n",
      "[epoch 68] train_loss: 0.071 train_accuracy: 0.973 val_accuracy: 0.943  recall: 0.881  f1: 0.889\n",
      "train loss:100%[**************************************************->]0.007\n",
      "[epoch 69] train_loss: 0.071 train_accuracy: 0.973 val_accuracy: 0.939  recall: 0.881  f1: 0.886\n",
      "train loss:100%[**************************************************->]0.179\n",
      "[epoch 70] train_loss: 0.068 train_accuracy: 0.974 val_accuracy: 0.943  recall: 0.896  f1: 0.877\n",
      "train loss:100%[**************************************************->]0.106\n",
      "[epoch 71] train_loss: 0.070 train_accuracy: 0.974 val_accuracy: 0.913  recall: 0.841  f1: 0.852\n",
      "train loss:100%[**************************************************->]0.088\n",
      "[epoch 72] train_loss: 0.068 train_accuracy: 0.975 val_accuracy: 0.942  recall: 0.894  f1: 0.886\n",
      "train loss:100%[**************************************************->]0.035\n",
      "[epoch 73] train_loss: 0.066 train_accuracy: 0.975 val_accuracy: 0.929  recall: 0.869  f1: 0.874\n",
      "train loss:100%[**************************************************->]0.280\n",
      "[epoch 74] train_loss: 0.065 train_accuracy: 0.975 val_accuracy: 0.943  recall: 0.898  f1: 0.894\n",
      "train loss:100%[**************************************************->]0.040\n",
      "[epoch 75] train_loss: 0.066 train_accuracy: 0.976 val_accuracy: 0.945  recall: 0.885  f1: 0.889\n",
      "train loss:100%[**************************************************->]0.006\n",
      "[epoch 76] train_loss: 0.063 train_accuracy: 0.976 val_accuracy: 0.873  recall: 0.740  f1: 0.753\n",
      "train loss:100%[**************************************************->]0.103\n",
      "[epoch 77] train_loss: 0.062 train_accuracy: 0.976 val_accuracy: 0.938  recall: 0.886  f1: 0.892\n",
      "train loss:100%[**************************************************->]0.005\n",
      "[epoch 78] train_loss: 0.063 train_accuracy: 0.976 val_accuracy: 0.947  recall: 0.889  f1: 0.897\n",
      "train loss:100%[**************************************************->]0.079\n",
      "[epoch 79] train_loss: 0.062 train_accuracy: 0.977 val_accuracy: 0.946  recall: 0.894  f1: 0.901\n",
      "train loss:100%[**************************************************->]0.112\n",
      "[epoch 80] train_loss: 0.064 train_accuracy: 0.975 val_accuracy: 0.921  recall: 0.880  f1: 0.835\n",
      "train loss:100%[**************************************************->]0.001\n",
      "[epoch 81] train_loss: 0.060 train_accuracy: 0.977 val_accuracy: 0.938  recall: 0.886  f1: 0.865\n",
      "train loss:100%[**************************************************->]0.003\n",
      "[epoch 82] train_loss: 0.063 train_accuracy: 0.976 val_accuracy: 0.909  recall: 0.826  f1: 0.834\n",
      "train loss:100%[**************************************************->]0.069\n",
      "[epoch 83] train_loss: 0.060 train_accuracy: 0.977 val_accuracy: 0.943  recall: 0.891  f1: 0.892\n",
      "train loss:100%[**************************************************->]0.010\n",
      "[epoch 84] train_loss: 0.059 train_accuracy: 0.977 val_accuracy: 0.920  recall: 0.867  f1: 0.868\n",
      "train loss:100%[**************************************************->]0.028\n",
      "[epoch 85] train_loss: 0.059 train_accuracy: 0.978 val_accuracy: 0.950  recall: 0.897  f1: 0.894\n",
      "train loss:100%[**************************************************->]0.103\n",
      "[epoch 86] train_loss: 0.060 train_accuracy: 0.978 val_accuracy: 0.944  recall: 0.885  f1: 0.891\n",
      "train loss:100%[**************************************************->]0.030\n",
      "[epoch 87] train_loss: 0.057 train_accuracy: 0.978 val_accuracy: 0.941  recall: 0.885  f1: 0.882\n",
      "train loss:100%[**************************************************->]0.099\n",
      "[epoch 88] train_loss: 0.057 train_accuracy: 0.978 val_accuracy: 0.927  recall: 0.874  f1: 0.864\n",
      "train loss:100%[**************************************************->]0.075\n",
      "[epoch 89] train_loss: 0.058 train_accuracy: 0.978 val_accuracy: 0.946  recall: 0.897  f1: 0.901\n",
      "train loss:100%[**************************************************->]0.010\n",
      "[epoch 90] train_loss: 0.055 train_accuracy: 0.979 val_accuracy: 0.947  recall: 0.897  f1: 0.884\n",
      "train loss:100%[**************************************************->]0.007\n",
      "[epoch 91] train_loss: 0.056 train_accuracy: 0.978 val_accuracy: 0.949  recall: 0.898  f1: 0.901\n",
      "train loss:100%[**************************************************->]0.014\n",
      "[epoch 92] train_loss: 0.057 train_accuracy: 0.979 val_accuracy: 0.954  recall: 0.899  f1: 0.902\n",
      "train loss:100%[**************************************************->]0.078\n",
      "[epoch 93] train_loss: 0.055 train_accuracy: 0.979 val_accuracy: 0.907  recall: 0.846  f1: 0.844\n",
      "train loss:100%[**************************************************->]0.099\n",
      "[epoch 94] train_loss: 0.054 train_accuracy: 0.979 val_accuracy: 0.952  recall: 0.890  f1: 0.899\n",
      "train loss:100%[**************************************************->]0.127\n",
      "[epoch 95] train_loss: 0.056 train_accuracy: 0.978 val_accuracy: 0.949  recall: 0.899  f1: 0.892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:100%[**************************************************->]0.003\n",
      "[epoch 96] train_loss: 0.052 train_accuracy: 0.980 val_accuracy: 0.954  recall: 0.906  f1: 0.905\n",
      "train loss:100%[**************************************************->]0.058\n",
      "[epoch 97] train_loss: 0.056 train_accuracy: 0.978 val_accuracy: 0.950  recall: 0.899  f1: 0.895\n",
      "train loss:100%[**************************************************->]0.002\n",
      "[epoch 98] train_loss: 0.054 train_accuracy: 0.979 val_accuracy: 0.949  recall: 0.887  f1: 0.889\n",
      "train loss:100%[**************************************************->]0.000\n",
      "[epoch 99] train_loss: 0.052 train_accuracy: 0.980 val_accuracy: 0.950  recall: 0.905  f1: 0.904\n",
      "train loss:100%[**************************************************->]0.029\n",
      "[epoch 100] train_loss: 0.055 train_accuracy: 0.979 val_accuracy: 0.952  recall: 0.900  f1: 0.895\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.categories = sorted(os.listdir(data_dir))\n",
    "        self.data = []\n",
    "        self.transform = transform\n",
    "        for category in self.categories:\n",
    "            category_dir = os.path.join(data_dir, category)\n",
    "            category_data = sorted(os.listdir(category_dir))\n",
    "            self.data.extend([(os.path.join(category_dir, file), self.categories.index(category)) for file in category_data])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_path, label = self.data[index]\n",
    "        data = np.load(file_path)\n",
    "        image = Image.fromarray(data.astype(np.uint8))\n",
    "        image = transform(image)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using {} device.\".format(device))\n",
    "\n",
    "transform = transforms.Compose([ transforms.Grayscale(num_output_channels = 1),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5), (0.5))])\n",
    "\n",
    "train_dataset = CustomDataset(\"features/train_npy\",transform=transform)\n",
    "train_num = len(train_dataset)\n",
    "dev_list = train_dataset.categories\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size, shuffle=True,\n",
    "                                               num_workers=0)\n",
    "\n",
    "validate_dataset = CustomDataset(\"features/val_npy\",transform=transform)\n",
    "val_num = len(validate_dataset)\n",
    "validate_loader = torch.utils.data.DataLoader(validate_dataset,\n",
    "                                                  batch_size=batch_size, shuffle=False,\n",
    "                                                  num_workers=0)\n",
    "\n",
    "print(\"using {} images for training, {} images for validation.\".format(train_num, val_num))\n",
    "\n",
    "net = MobileNetV2(num_classes=31)\n",
    "net.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "epochs = 100\n",
    "save_path = './MobileNet_parameters.pth'\n",
    "best_f1 = 0.0\n",
    "train_accurate_list = []\n",
    "val_accurate_list = []\n",
    "f1_list = []\n",
    "recall_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    for step, data in enumerate(train_loader, start=0):\n",
    "        images, labels = data\n",
    "        images = images.reshape(images.shape[0], 1, 1500)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images.to(device))\n",
    "        predict_y = torch.max(outputs, dim=1)[1]\n",
    "        train_acc += torch.eq(predict_y, labels.to(device)).sum().item()\n",
    "        loss = loss_function(outputs,labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        rate = (step + 1) / len(train_loader)\n",
    "        a = \"*\" * int(rate * 50)\n",
    "        b = \".\" * int((1 - rate) * 50)\n",
    "        print(\"\\rtrain loss:{:^3.0f}%[{}->{}]{:.3f}\".format(int(rate * 100), a, b, loss), end=\"\")\n",
    "    print()\n",
    "    train_accurate = train_acc / train_num\n",
    "    train_accurate_list.append(train_accurate)\n",
    "    net.eval()\n",
    "    acc = 0.0 \n",
    "    val = torch.tensor([])\n",
    "    pre = torch.tensor([])\n",
    "    with torch.no_grad():\n",
    "        for val_data in validate_loader:\n",
    "            val_images, val_labels = val_data\n",
    "            val_images = val_images.reshape(val_images.shape[0], 1, 1500)\n",
    "            outputs = net(val_images.to(device))\n",
    "            predict_y = torch.max(outputs, dim=1)[1]\n",
    "            pre = torch.cat([pre.to(device), predict_y.to(device)])\n",
    "            val = torch.cat([val.to(device), val_labels.to(device)])\n",
    "            acc += torch.eq(predict_y, val_labels.to(device)).sum().item()\n",
    "    val_accurate = acc / val_num\n",
    "    val_accurate_list.append(val_accurate)\n",
    "    f1 = f1_score(val.cpu(), pre.cpu(), average='macro')\n",
    "    recall = recall_score(val.cpu(), pre.cpu(), average='macro')\n",
    "\n",
    "    f1_list.append(f1)\n",
    "    recall_list.append(recall)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_pre = pre\n",
    "        best_val = val\n",
    "        torch.save(net.state_dict(), save_path)\n",
    "        torch.save(best_pre, 'pre_val_label/best_pre_MobileNet.pt')\n",
    "        torch.save(best_val, 'pre_val_label/best_val_MobileNet.pt')\n",
    "    print('[epoch %d] train_loss: %.3f train_accuracy: %.3f val_accuracy: %.3f  recall: %.3f  f1: %.3f' %\n",
    "          (epoch + 1, running_loss / step, train_accurate, val_accurate, recall, f1))\n",
    "    with open(\"MobileNet_result_npy.txt\", 'a') as file:\n",
    "        file.write(\"[epoch \" + str(epoch + 1) + \"]\" + \"  \" + \"train_accuracy:\" + str(train_accurate) + \"  \" + \"val_accuracy:\" + str(val_accurate) + \"  \" + \"recall:\" + str(recall) + \"  \" + \"f1:\" + str(f1) + '\\n')\n",
    "print('Finished Training')\n",
    "\n",
    "iterations = range(1, len(train_accurate_list) + 1)\n",
    "with open(\"MobileNet_npy_plt_data.txt\", 'a') as file:\n",
    "    file.write(\"iterations:\" + str(iterations) +\n",
    "               \"train_accurate_list:\" + str(train_accurate_list) +\n",
    "               \"val_accurate_list:\" + str(val_accurate_list) +\n",
    "               \"f1_list:\" + str(f1_list) +\n",
    "               \"recall_list:\" + str(recall_list) +\n",
    "               \"dev_list:\" + str(dev_list) + '\\n')\n",
    "conf_matrix = confusion_matrix(best_val.cpu(),best_pre.cpu())\n",
    "plot_matrix(conf_matrix,dev_list,\"MobileNet_confusion_matrix_npy.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920a9528",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
